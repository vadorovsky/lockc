<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>lockc Documentation</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="**lockc** is open source software for providing MAC (Mandatory Access Control) implemented in Rust">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="containers-do-not-contain.html"><strong aria-hidden="true">2.</strong> Containers do not contain</a></li><li class="chapter-item expanded "><a href="architecture.html"><strong aria-hidden="true">3.</strong> Architecture</a></li><li class="chapter-item expanded "><a href="configuration.html"><strong aria-hidden="true">4.</strong> Getting started</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="build/index.html"><strong aria-hidden="true">4.1.</strong> Build</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="build/dapper.html"><strong aria-hidden="true">4.1.1.</strong> Dapper</a></li><li class="chapter-item expanded "><a href="build/cargo.html"><strong aria-hidden="true">4.1.2.</strong> Cargo</a></li><li class="chapter-item expanded "><a href="build/container-image.html"><strong aria-hidden="true">4.1.3.</strong> Container image</a></li></ol></li><li class="chapter-item expanded "><a href="install/index.html"><strong aria-hidden="true">4.2.</strong> Install</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="install/docker.html"><strong aria-hidden="true">4.2.1.</strong> With Docker</a></li><li class="chapter-item expanded "><a href="install/kubernetes.html"><strong aria-hidden="true">4.2.2.</strong> With Kubernetes</a></li></ol></li><li class="chapter-item expanded "><a href="terraform/index.html"><strong aria-hidden="true">4.3.</strong> Development environment (Terraform)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="terraform/libvirt.html"><strong aria-hidden="true">4.3.1.</strong> libvirt</a></li><li class="chapter-item expanded "><a href="terraform/openstack.html"><strong aria-hidden="true">4.3.2.</strong> OpenStack</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="policies/index.html"><strong aria-hidden="true">5.</strong> Policies</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="policies/file-access.html"><strong aria-hidden="true">5.1.</strong> File access</a></li><li class="chapter-item expanded "><a href="policies/mount.html"><strong aria-hidden="true">5.2.</strong> Mount</a></li><li class="chapter-item expanded "><a href="policies/syslog.html"><strong aria-hidden="true">5.3.</strong> Syslog</a></li></ol></li><li class="chapter-item expanded "><a href="tuning/index.html"><strong aria-hidden="true">6.</strong> Tuning</a></li><li class="chapter-item expanded "><a href="demos/index.html"><strong aria-hidden="true">7.</strong> Demos</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="demos/mount.html"><strong aria-hidden="true">7.1.</strong> Mount</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">lockc Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/rancher-sandbox/lockc" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p><img src="/images/logo-horizontal-lockc.png" alt="lockc" /></p>
<h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p><strong>lockc</strong> is open source software for providing MAC (Mandatory Access Control)
type of security audit for container workloads.</p>
<p>The main reason why <strong>lockc</strong> exists is that <strong>containers do not contain</strong>.
Containers are not as secure and isolated as VMs. By default, they expose
a lot of information about host OS and provide ways to &quot;break out&quot; from the
container. <strong>lockc</strong> aims to provide more isolation to containers and make them
more secure.</p>
<p>The <a href="containers-do-not-contain.html">Containers do not contain</a> documentation
section explains why we mean by that phrase and what kind of behavior we want
to restrict with <strong>lockc</strong>.</p>
<p>The main technology behind lockc is <a href="https://ebpf.io/">eBPF</a> - to be more
precise, its ability to attach to <a href="https://www.kernel.org/doc/html/latest/bpf/bpf_lsm.html">LSM hooks</a></p>
<p>Please note that currently lockc is an experimental project, not meant for
production environments. Currently we don't publish any official binaries or
packages to use, except of a Rust crate. Currently the most convenient way
to use it is to use the source code and follow the guide.</p>
<h2 id="contributing"><a class="header" href="#contributing">Contributing</a></h2>
<p>If you need help or want to talk with contributors, please come chat with
us on <code>#lockc</code> channel on the <a href="https://discord.gg/799cmsYB4q">Rust Cloud Native Discord server</a>.</p>
<p>You can find the source code on <a href="https://github.com/rancher-sandbox/lockc">GitHub</a>
and issues and feature requests can be posted on the
<a href="https://github.com/rancher-sandbox/lockc/issues">GitHub issue tracker</a>.
<strong>lockc</strong> relies on the community to fix bugs and add features: if you'd like
to contribute, please read the <a href="https://github.com/rancher-sandbox/lockc/blob/master/CONTRIBUTING.md">CONTRIBUTING</a>
guide and consider opening <a href="https://github.com/rancher-sandbox/lockc/pulls">pull request</a>.</p>
<h2 id="license"><a class="header" href="#license">License</a></h2>
<p><strong>lockc's</strong> userspace part is licensed under <a href="https://github.com/rancher-sandbox/lockc/blob/main/LICENSE">Apache License, version 2.0</a>.</p>
<p>eBPF programs inside <a href="https://github.com/rancher-sandbox/lockc/tree/main/lockc/src/bpf">lockc/src/bpf directory</a>
are licensed under <a href="https://github.com/rancher-sandbox/lockc/blob/main/lockc/src/bpf/LICENSE">GNU General Public License, version 2</a>.</p>
<p>Documentation is licensed under <a href="https://www.mozilla.org/MPL/2.0/">Mozilla Public License v2.0</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="containers-do-not-contain"><a class="header" href="#containers-do-not-contain">Containers do not contain</a></h1>
<p>Many people assume that containers:</p>
<ul>
<li>provide the same or similar isolation to virtual machines</li>
<li>protects the host system</li>
<li>sandboxes applications</li>
</ul>
<p>While all the points except the first one are partially true, some parts of the
host filesystems are still exposed by default to containers and there are ways to
gain full access.</p>
<p>This section highlights and explains problematic exploitation possibilities
that <strong>lockc</strong> aims to fix via policies.</p>
<p>Please note that as <strong>lockc</strong> is still in early development stage, it doesn't
protect against all examples provided at this time. However, covering them all
is in the roadmap.</p>
<p>The goal of <strong>lockc</strong> is to eventually prevent any of those examples to be done
by a regular user. Following some examples as root by explicitly choosing the
<em>privileged</em> policy level in lockc is going to be still allowed. However, it is
is discouraged to use the <em>priviliged</em> level for containers which are not part
of Kubernetes infra (CNI plugins, operators, network meshes etc.). We might
still consider restricting some of behaviors even for <em>privileged</em> (i.e. it's
probably hard to justify <code>chroot</code> inside containers under any ciricumstance).</p>
<h2 id="not-everything-is-namespaced"><a class="header" href="#not-everything-is-namespaced">Not everything is namespaced</a></h2>
<p>Despite the fact that containers come with their own rootfs, some parts of the
filesystem are <strong>not namespaced</strong>, which means that the content of some
directories is <strong>exactly the same as on the host OS</strong>. Examples:</p>
<ul>
<li>Kernel filesystems under <em>/sys</em></li>
<li>many sysctls under <em>/proc/sys</em></li>
</ul>
<p>For non-privileged containers, the content of those directories is read-only.
However, privileged containers can write to them. In both cases, we think that
even exposing many of those directories without write access is unnecessary
for regular containers.</p>
<p>To show some more concrete examples, access to those directories can allow to:</p>
<ul>
<li>Check and change GPU settings</li>
</ul>
<pre><code class="language-bash">❯ docker run --rm -it opensuse/tumbleweed:latest bash
f4891490a2f3:/ # cat /sys/class/drm/card0/device/power_dpm_force_performance_level
auto
f4891490a2f3:/ # exit
❯ docker run --rm --privileged -it opensuse/tumbleweed:latest bash
bad479286479:/ # echo high &gt; /sys/class/drm/card0/device/power_dpm_force_performance_level
bad479286479:/ # cat /sys/class/drm/card0/device/power_dpm_force_performance_level
high
bad479286479:/ # exit
❯ cat /sys/class/drm/card0/device/power_dpm_force_performance_level
high
</code></pre>
<ul>
<li>look at the host OS filesystem metadata</li>
</ul>
<pre><code class="language-bash">❯ docker run --rm -it opensuse/tumbleweed:latest bash
0d35122d08f9:~ # ls /sys/fs/btrfs/a8222a26-d11e-4276-9c38-9df2812cead2/
allocation  bdi  bg_reclaim_threshold  checksum  clone_alignment  devices  devinfo  exclusive_operation  features  generation  label  metadata_uuid  nodesize  qgroups  quota_override  read_policy  sectorsize
</code></pre>
<ul>
<li>use fdisk in a privileged container</li>
</ul>
<pre><code class="language-bash">❯ docker run --rm -it --privileged registry.opensuse.org/opensuse/toolbox:latest bash
8b71e0119552:/ # fdisk -l
Disk /dev/nvme0n1: 1.82 TiB, 2000398934016 bytes, 3907029168 sectors
Disk model: Samsung SSD 970 EVO Plus 2TB
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: gpt
Disk identifier: 8EEBDAB8-F965-4BA0-918A-2671BC67117C

Device           Start        End    Sectors  Size Type
/dev/nvme0n1p1    2048    1026047    1024000  500M EFI System
/dev/nvme0n1p2 1026048 3907029134 3906003087  1.8T Linux filesystem
</code></pre>
<h2 id="host-mounts"><a class="header" href="#host-mounts">Host mounts</a></h2>
<p>Container engines allow to bind mount any directory from the host. When using
local, non-clusterized container engines (docker, podman etc.) there are no
restrictions about what can be mounted. In case of Docker, anyone who has an
access to the socket (usually a member of <code>docker</code> group) can mount anything.</p>
<p>That gives every member of the <code>docker</code> group an access to the host OS as root:</p>
<pre><code class="language-bash">❯ docker run --rm --privileged -it -v /:/rootfs opensuse/tumbleweed:latest bash
efa4f6e0529a:/ # chroot /rootfs
sh-4.4#
</code></pre>
<p>The <code>chroot</code> works without <code>--privileged</code> as well:</p>
<pre><code class="language-bash">❯ docker run --rm -it -v /:/rootfs opensuse/tumbleweed:latest bash
abb67212044d:/ # chroot /rootfs
sh-4.4#
</code></pre>
<p>The other approach is to mount a Docker socket. The image used here is <code>docker</code>
which is the official image with Docker binaries installed. After starting the
first container, we are able to list containers running on the host. Then, we
are able to run another container - from inside the first one - which is
mounting directories from the host</p>
<pre><code class="language-bash">❯ docker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock docker sh
/ # docker ps
CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS     NAMES
066811b60d69   docker    &quot;docker-entrypoint.s…&quot;   5 seconds ago   Up 5 seconds             suspicious_liskov
/ # docker run --rm --privileged -it opensuse/tumbleweed:latest bash
fcb94c1d3af6:/ # exit
/ # docker run --rm --privileged -it -v /:/rootfs opensuse/tumbleweed:latest bash
54b08e30fd9e:/ # chroot /rootfs
sh-4.4# cat /etc/os-release
NAME=&quot;openSUSE Leap&quot;
VERSION=&quot;15.3&quot;
ID=&quot;opensuse-leap&quot;
ID_LIKE=&quot;suse opensuse&quot;
VERSION_ID=&quot;15.3&quot;
PRETTY_NAME=&quot;openSUSE Leap 15.3&quot;
ANSI_COLOR=&quot;0;32&quot;
CPE_NAME=&quot;cpe:/o:opensuse:leap:15.3&quot;
BUG_REPORT_URL=&quot;https://bugs.opensuse.org&quot;
HOME_URL=&quot;https://www.opensuse.org/&quot;
</code></pre>
<p>Notice the difference between Linux distibution versions. The second container
image we used is <em>openSUSE Tumbleweed</em>, but the host is running
<em>openSUSE Leap 15.3</em>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="architecture"><a class="header" href="#architecture">Architecture</a></h1>
<p>The project consists of 3 parts:</p>
<ul>
<li>the set of BPF programs (written in C)
<ul>
<li>programs for monitoring processes, which detects whether new processes
are running inside any container, which means applying policies on them</li>
<li>programs attached to particular LSM hooks, which allow or deny actions
based on the policy applied to the container (currently all containers have
the <code>baseline</code> policy applied, the mechanism of differentiating between
policies per container/pod is yet to be implemented)</li>
</ul>
</li>
<li><strong>lockcd</strong> - the userspace program (written in Rust)
<ul>
<li>loads the BPF programs into the kernel, pins them in BPFFS</li>
<li>monitors runc processes, registers new containers and determines which
policy should be applied to a container</li>
<li>in future, it's going to serve as the configuration manager and log
collector</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting started</a></h1>
<ul>
<li><a href="build/README.html">Build</a> - How to build lockc from the sources</li>
<li><a href="install/README.html">Install</a> - Configuring and installing lockc</li>
<li><a href="terraform/README.html">Development environment (Terraform)</a> - Using Terraform for setting up development environment lockc</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="building-lockc"><a class="header" href="#building-lockc">Building lockc</a></h1>
<p>The first step to try out lockc is to build it. There are several ways to do
that:</p>
<ul>
<li><strong><a href="build/dapper.html">Dapper</a></strong> - build binaries within container
<ul>
<li>doesn't require any dependencies on the host system (except Docker)</li>
<li>ensures that Rust, Cargo and all dependencies are in the newest version</li>
<li>ensures the same behavior as on CI, aims to reduce &quot;it worked on machine&quot;
kind of problems</li>
<li>recommended for
<ul>
<li>trying out the project (especially if there is no interest in changing
the code)</li>
<li>final build before submitting changes in the code</li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="build/cargo.html">Cargo</a></strong> - build binaries with Cargo (Rust build system) on the host
<ul>
<li>convenient for local development, IDE/editor integration</li>
</ul>
</li>
<li><strong><a href="build/container-image.html">Container image</a></strong> - build a container image which can be deployed on
Kubernetes
<ul>
<li>the only method to try lockc on Kubernetes</li>
<li>doesn't work for Docker integration, where we rather install lockc as a
binary on the host, managed by systemd</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dapper"><a class="header" href="#dapper">Dapper</a></h1>
<h2 id="building-lockc-1"><a class="header" href="#building-lockc-1">Building lockc</a></h2>
<p>One option for building lockc is using dapper to perform the build inside
container, without installing needed dependencies on the host system.</p>
<p>This guide assumes that you have <code>docker</code> or any other container engine
installed.</p>
<p>The first step is to install dapper, if it's not present. It can be done
either by downloading a binary:</p>
<pre><code class="language-bash">curl -sL https://releases.rancher.com/dapper/latest/dapper-$(uname -s)-$(uname -m) &gt; /usr/local/bin/dapper
chmod +x /usr/local/bin/dapper
</code></pre>
<p>Or by using <code>go</code>:</p>
<pre><code class="language-bash">go install github.com/rancher/dapper@latest
</code></pre>
<p>Dapper should be launched always in the main directory of the project, where
<code>Dockerfile.dapper</code> file is present.</p>
<p>Our build container image has no entrypoint, so calling <code>dapper</code> without any
argument is spawning a shell inside the container:</p>
<pre><code class="language-bash">$ dapper
[...]
root@ea133ef3d28e:/source#
</code></pre>
<p>Usually we will be interested in using <code>cargo</code> inside the container spawned by
dapper.
<a href="build/cargo.html">More information about cargo can be found here.</a></p>
<p>The build (of both BPF and userspace part) can be performed by running the
following command:</p>
<pre><code class="language-bash">dapper cargo build
</code></pre>
<p>A successful build should result in binaries being present in <code>target/debug</code>
directory.</p>
<p>Running tests:</p>
<pre><code class="language-bash">dapper cargo test
</code></pre>
<p>Running lints:</p>
<pre><code class="language-bash">dapper cargo clippy
</code></pre>
<h2 id="building-tarball-with-binary-and-unit"><a class="header" href="#building-tarball-with-binary-and-unit">Building tarball with binary and unit</a></h2>
<p>To make distribution of lockc for Docker users easier, we have a possibility of
building an archive with binary and systemd unit which can be just unpacked in
<code>/</code> directory. It can be done by the following command:</p>
<pre><code class="language-bash">dapper cargo xtask bintar
</code></pre>
<p>By default it archives lockcd binary in <code>usr/local/bin</code>, but the
destination directory can be changed by the following arguments:</p>
<ul>
<li><code>--prefix</code> - prefix of the most of installation destinations, default:
<code>usr/local</code></li>
<li><code>--bindir</code> - directory for binary files, default: <code>bin</code></li>
<li><code>--unitdir</code> - directory for systemd units, default: <code>lib/systemd/system</code></li>
<li><code>--sysconfdir</code> - directory for configuration files, default: <code>etc</code></li>
</ul>
<p>By default, binaries are installed from the <code>debug</code> target profile. If you want
to change it, use the <code>--profile</code> argument. <code>--profile release</code> is what you
most likely want to use when creating a tarball for releases and production
systems.</p>
<p>The resulting binary should be available as <code>target/[profile]/lockc.tar.gz</code>
(i.e. <code>target/debug/lockc.tar.gz</code>).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cargo"><a class="header" href="#cargo">Cargo</a></h1>
<p>If you are comfortable with installing all dependencies on your host system,
you need to install the following software:</p>
<ul>
<li>LLVM</li>
<li>libbpf, bpftool</li>
<li>Rust, Cargo</li>
</ul>
<h2 id="llvm"><a class="header" href="#llvm">LLVM</a></h2>
<p>We need a recent version of LLVM (at least 12) to build BPF programs.</p>
<p>LLVM has an official <a href="https://apt.llvm.org/">apt repository</a> with recent
stable versions.</p>
<p>Distributions with up to date software repositories like Arch, Fedora, openSUSE
Tumbleweed are shipping recent versions of LLVM.</p>
<p>In more stable and not up to date distributions (CentOS, openSUSE Leap, RHEL,
SLES), using some kind of development repository might be an option. For
example, openSUSE Leap users can use the following devel repo:</p>
<pre><code class="language-bash">zypper ar -r -p 90 https://download.opensuse.org/repositories/devel:/tools:/compiler/openSUSE_Leap_15.3/devel:tools:compiler.repo
zypper ref
zypper up --allow-vendor-change
zypper in clang llvm
</code></pre>
<p>If there is no packaging of recent LLVM versions for your distribution, there
is also an option to <a href="https://releases.llvm.org/download.html">download binaries</a>.</p>
<h2 id="libbpf-bpftool"><a class="header" href="#libbpf-bpftool">libbpf, bpftool</a></h2>
<p>libbpf is the official C library for writing, loading and managing BPF programs
and entities. bpftool is the official CLI for interacting with BPF subsystem.</p>
<p>Distributions with up to date software (Arch, Fedora, openSUSE Tumbleweed)
usually provide packaging for both.</p>
<p>Especially for more stable and less up to date distributions, but even
generally, we would recommend to build both dependencies from source. Both of
them are the part of the Linux kernel source.</p>
<p>The easiest way to get the kernel source is to download a tarball available on
<a href="https://www.kernel.org/">kernel.org</a>. Then build and install tools from it
(the version might vary from this snippet):</p>
<pre><code class="language-bash">tar -xvf linux-5.14.9.tar.xz
cd linux-5.14.9
cd tools/lib/bpf
make -j $(nproc)
make install prefix=/usr
cd ../../bpf/bpftool
make -j $(nproc)
make install prefix=/usr
</code></pre>
<p>If you are interested in tracking the history of Linux kernel source and/or are
comfortable using git for it, you can clone one of the git trees:</p>
<ul>
<li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/">stable tree</a> -
stable releases and release candidates, this is where the tarball comes from</li>
<li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/">mainline tree</a> -
patches accepted by Linus, release candidates</li>
<li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next.git/">bpf-next tree</a> -
development of BPF features, before being mainlined</li>
<li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/">bpf tree</a> -
BPF bugfixes which are backported to the stable tree</li>
</ul>
<p>Assuming you want to use the stable tree:</p>
<pre><code class="language-bash">git clone git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git
cd linux
git tag -l # List available tags
git checkout v5.14.9 # Check out to whatever is the newest
cd tools/lib/bpf
make -j $(nproc)
make install prefix=/usr
cd ../../bpf/bpftool
make -j $(nproc)
make install prefix=/usr
</code></pre>
<h2 id="installing-rust"><a class="header" href="#installing-rust">Installing Rust</a></h2>
<p>Our recommended way of installing Rust is using <strong>rustup</strong>.
<a href="https://rustup.rs/">Their website</a> contains installation instruction.</p>
<p>After installing rustup, let's install lint tools:</p>
<pre><code class="language-bash">rustup component add clippy rustfmt
</code></pre>
<p>And then cargo-libbpf, needed for building the BPF part:</p>
<pre><code class="language-bash">cargo install libbpf-cargo
</code></pre>
<h2 id="building-lockc-2"><a class="header" href="#building-lockc-2">Building lockc</a></h2>
<p>After installing all needed dependencies, it's time to build lockc.</p>
<p>The build of the project can be done with:</p>
<pre><code class="language-bash">cargo build
</code></pre>
<p>Running tests:</p>
<pre><code class="language-bash">cargo test
</code></pre>
<p>Running lints:</p>
<pre><code class="language-bash">cargo clippy
</code></pre>
<h2 id="installing-lockc"><a class="header" href="#installing-lockc">Installing lockc</a></h2>
<p>To install lockc on your host, use the following command:</p>
<pre><code class="language-bash">cargo xtask install
</code></pre>
<p>Do not run this command with sudo! Why?</p>
<p>tl;dr: you will be asked for password when necessary, don't worry!</p>
<p>Explanation: Running cargo with sudo ends with weird consequences like not
seing cargo content from your home directory or leaving some files owned by
root in <code>target</code>. When any destination directory is owned by root, sudo will
be launched automatically by <code>xtask install</code> just to perform necessary
installation steps.</p>
<p>By default it tries to install lockcd binary in <code>/usr/local/bin</code>, but the
destination directory can be changed by the following arguments:</p>
<ul>
<li><code>--destdir</code> - the rootfs of your system, default: <code>/</code></li>
<li><code>--prefix</code> - prefix of the most of installation destinations, default:
<code>usr/local</code></li>
<li><code>--bindir</code> - directory for binary files, default: <code>bin</code></li>
<li><code>--unitdir</code> - directory for systemd units, default: <code>lib/systemd/system</code></li>
<li><code>--sysconfdir</code> - directory for configuration files, default: <code>etc</code></li>
</ul>
<p>By default, binaries are installed from the <code>debug</code> target profile. If you want
to change it, use the <code>--profile</code> argument. <code>--profile release</code> is what you
most likely want to use when packaging or installing on the production system.</p>
<h2 id="building-tarball-with-binary-and-unit-1"><a class="header" href="#building-tarball-with-binary-and-unit-1">Building tarball with binary and unit</a></h2>
<p>To make distribution of lockc for Docker users easier, we have a possibility of
building an archive with binary and systemd unit which can be just unpacked in
<code>/</code> directory. It can be done by the following command:</p>
<pre><code class="language-bash">cargo xtask bintar
</code></pre>
<p>By default it archives lockcd binary in <code>usr/local/bin</code>, but the
destination directory can be changed by the following arguments:</p>
<ul>
<li><code>--prefix</code> - prefix of the most of installation destinations, default:
<code>usr/local</code></li>
<li><code>--bindir</code> - directory for binary files, default: <code>bin</code></li>
<li><code>--unitdir</code> - directory for systemd units, default: <code>lib/systemd/system</code></li>
<li><code>--sysconfdir</code> - directory for configuration files, default: <code>etc</code></li>
</ul>
<p>By default, binaries are installed from the <code>debug</code> target profile. If you want
to change it, use the <code>--profile</code> argument. <code>--profile release</code> is what you
most likely want to use when creating a tarball for releases and production
systems.</p>
<p>The resulting binary should be available as <code>target/[profile]/lockc.tar.gz</code>
(i.e. <code>target/debug/lockc.tar.gz</code>).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="container-image"><a class="header" href="#container-image">Container image</a></h1>
<p>lockc repository contains a <code>Dockerfile</code> which can be used for building a
container image. The main purpose of building it is ability to deploy lockc on
Kubernetes.</p>
<p>Building a local image can be done in a basic way, like:</p>
<pre><code class="language-bash">docker build -t lockcd .
</code></pre>
<p>For quick development and usage of the image on different (virtual) machines,
it's convenient to use <a href="https://ttl.sh/">ttl.sh</a> which is an anonymous and
ephemeral container image registry.</p>
<p>To build and push an image to ttl.sh, you can use the following commands:</p>
<pre><code class="language-bash">export IMAGE_NAME=$(uuidgen)
docker build -t ttl.sh/${IMAGE_NAME}:30m .
docker push ttl.sh/${IMAGE_NAME}:30m
</code></pre>
<p>After building the container image, you will be able to
<a href="build/../install/kubernetes.html">install lockc on Kubernetes</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="install"><a class="header" href="#install">Install</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="with-docker"><a class="header" href="#with-docker">With Docker</a></h1>
<p>This documentation section explains how to install lockc on a single machine
with Docker. In order to do that, we need to install <code>lockcd</code> binary and a
systemd unit for it.</p>
<h2 id="installation-methods"><a class="header" href="#installation-methods">Installation methods</a></h2>
<p>There are two ways to do that.</p>
<h3 id="install-with-cargo"><a class="header" href="#install-with-cargo">Install with cargo</a></h3>
<p>If you want to install lockc on a machine where you have the source code of
lockc, you can do it with cargo. You need to build lockc with Cargo before
that. After building lockc, you can install it with the following command.</p>
<pre><code class="language-bash">cargo xtask install
</code></pre>
<p>Do not run this command with sudo! Why?</p>
<p>tl;dr: you will be asked for password when necessary, don't worry!</p>
<p>Explanation: Running cargo with sudo ends with weird consequences like not
seing cargo content from your home directory or leaving some files owned by
root in <code>target</code>. When any destination directory is owned by root, sudo will
be launched automatically by <code>xtask install</code> just to perform necessary
installation steps.</p>
<p>By default it tries to install lockcd binary in <code>/usr/local/bin</code>, but the
destination directory can be changed by the following arguments:</p>
<ul>
<li><code>--destdir</code> - the rootfs of your system, default: <code>/</code></li>
<li><code>--prefix</code> - prefix of the most of installation destinations, default:
<code>usr/local</code></li>
<li><code>--bindir</code> - directory for binary files, default: <code>bin</code></li>
<li><code>--unitdir</code> - directory for systemd units, default: <code>lib/systemd/system</code></li>
<li><code>--sysconfdir</code> - directory for configuration files, default: <code>etc</code></li>
</ul>
<p>By default, binaries are installed from the <code>debug</code> target profile. If you want
to change it, use the <code>--profile</code> argument. <code>--profile release</code> is what you
most likely want to use when packaging or installing on the production system.</p>
<h3 id="unpack-the-bintar"><a class="header" href="#unpack-the-bintar">Unpack the bintar</a></h3>
<p>Documentation sections about:</p>
<ul>
<li><a href="install/../build/dapper.html">building with Dapper</a></li>
<li><a href="install/../build/cargo.html">building with Cargo</a></li>
</ul>
<p>mention <em>Building tarball with binary and unit</em>. To quickly sum it up, you can
build a &quot;bintar&quot; by doing:</p>
<pre><code class="language-bash">dapper cargo xtask bintar
</code></pre>
<p>or:</p>
<pre><code class="language-bash">cargo xtask bintar
</code></pre>
<p>Both commands will produce a bintar available as <code>target/[profile]/lockc.tar.gz</code>
(i.e. <code>target/debug/lockc.tar.gz</code>).</p>
<p>That tarball can be copied to any machine and unpacked with the following
command:</p>
<pre><code class="language-bash">sudo tar -C / -xzf lockc.tar.gz
</code></pre>
<h2 id="verify-the-installation"><a class="header" href="#verify-the-installation">Verify the installation</a></h2>
<p>After installing lockc, you should be able to enable and start the lockcd
service:</p>
<pre><code class="language-bash">sudo systemctl enable --now lockcd
</code></pre>
<p>After starting the service, you can verify that lockc is running by trying to
run a &quot;not containing&quot; container, like:</p>
<pre><code class="language-bash">$ docker run --rm -it -v /:/rootfs registry.opensuse.org/opensuse/toolbox:latest
docker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: rootfs_linux.go:76: mounting &quot;/&quot; to rootfs at &quot;/rootfs&quot; caused: mount through procfd: operation not permitted: unknown.
ERRO[0020] error waiting for container: context canceled
</code></pre>
<p>Or you can try to run a less insecure container and try to <code>ls</code> the contents
of <code>/sys</code>:</p>
<pre><code class="language-bash">$ docker run --rm -it registry.opensuse.org/opensuse/toolbox:latest
9b34d760017f:/ # ls /sys
ls: cannot open directory '/sys': Operation not permitted
9b34d760017f:/ # ls /sys/fs/btrfs
ls: cannot access '/sys/fs/btrfs': No such file or directory
9b34d760017f:/ # ls /sys/fs/cgroup
blkio  cpu,cpuacct  cpuset   freezer  memory  net_cls           net_prio    pids  systemd
cpu    cpuacct      devices  hugetlb  misc    net_cls,net_prio  perf_event  rdma
</code></pre>
<p>You should be able to see cgroups (which is fine), but other parts of <em>/sys</em>
should be hidden.</p>
<p>However, running insecure containers as root with <code>privileged</code> policy level
should work:</p>
<pre><code class="language-bash">$ sudo -i
# docker run --label org.lockc.policy=privileged --rm -it -v /:/rootfs registry.opensuse.org/opensuse/toolbox:latest bash
8ea310609fce:/ # 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="on-kubernetes"><a class="header" href="#on-kubernetes">On Kubernetes</a></h1>
<p>This section explains how to install lockc on a Kubernetes cluster with
<a href="https://helm.sh/">helm</a>.</p>
<p>The helm chart is available on <a href="https://github.com/rancher-sandbox/lockc-helm-charts">lockc-helm-chart</a> git repository.
Installation with default values can be done with:</p>
<pre><code class="language-bash">repo add lockc https://rancher-sandbox.github.io/lockc-helm-charts/
helm install install --create-namespace -n lockc lockc lockc/lockc
</code></pre>
<p>More info on lockc helm chart installation can be found <a href="https://rancher-sandbox.github.io/lockc-helm-charts">here</a></p>
<p>To use your own container image, you can override values. Please refer to the
<a href="install/../build/container-image.html">Container image</a> section for instructions about
building container images with lockc. Let's assume that you pushed an image
with lockc to <code>ttl.sh/caa530ed-1371-43f7-a9ad-293a4f930f83:30m</code>. In that case,
installation with that image can be done with the following command:</p>
<pre><code class="language-bash">helm install lockc lockc/lockc --namespace lockc \
    --set lockcd.image.repository=ttl.sh/caa530ed-1371-43f7-a9ad-293a4f930f83 \
    --set lockcd.image.tag=30m
</code></pre>
<p>Enabling debug logs can be helpful for troubleshooting or development. That can
be done with the following command:</p>
<pre><code class="language-bash">helm install lockc lockc/lockc/ --namespace lockc \
    --set lockcd.image.repository=ttl.sh/caa530ed-1371-43f7-a9ad-293a4f930f83 \
    --set lockcd.image.tag=30m \
    --set lockcd.debug.enabled=true
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="development-environment-terraform"><a class="header" href="#development-environment-terraform">Development environment (Terraform)</a></h1>
<p>There is also a possibility to run lockc built from source in virtual machines.</p>
<p>We support two virtual machine providers:</p>
<ul>
<li><strong><a href="terraform/libvirt.html">libvirt</a></strong> - Configure and start VMs in libvirt environment, locally</li>
<li><strong><a href="terraform/openstack.html">OpenStack</a></strong> - Configure and start VMs in OpenStack environment, on the cloud</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="libvirt"><a class="header" href="#libvirt">libvirt</a></h1>
<h2 id="configure-libvirt"><a class="header" href="#configure-libvirt">Configure libvirt</a></h2>
<p>VMs which we are going to run are using 9p to mount the source tree. To
ensure that those mounts are going to work correctly, open the
<code>/etc/libvirt/qemu.conf</code> file and ensure that the following options
are present there:</p>
<pre><code class="language-bash">user = &quot;root&quot;
group = &quot;root&quot;
dynamic_ownership = 0
</code></pre>
<p>If you had to edit the configuration, save the file and restart libvirt:</p>
<pre><code class="language-bash">sudo systemctl restart libvirtd
</code></pre>
<h2 id="running-vms"><a class="header" href="#running-vms">Running VMs</a></h2>
<p>Now it's time to prepare Terraform environment.</p>
<pre><code class="language-bash">cd contrib/terraform/libvirt
cp terraform.tfvars.json.example terraform.tfvars.json
</code></pre>
<p>After that, open the <code>terraform.tfvars.json</code> file with your favorite text
editor. The only setting which you really need to change is
<code>authorized_keys</code>. Please paste your public SSH key there. Otherwise,
connecting to VMs with SSH will be impossible.</p>
<p>By default, the <code>enable_k8s_containerd</code> option is <code>true</code> which means that
Kubernetes cluster with CRI containerd is going to be deployed on VMs.</p>
<p>If you want to rather use lockc with Docker as a local container engine, set
<code>enable_k8s_containerd</code> to <code>false</code> and set <code>enable_docker</code> to <code>true</code>. In that
case, it also makes sense to set <code>control_planes</code> to <code>1</code> and <code>workers</code> to <code>0</code>,
since one VM is enough for non-clusterized Docker.</p>
<p>Other customization available through <code>terraform.tfvars.json</code> are explained
further in this document.</p>
<p>After setting the options, initialize the environment with:</p>
<pre><code class="language-bash">terraform init
</code></pre>
<p>And then start the VMs:</p>
<pre><code class="language-bash">terraform apply
</code></pre>
<p>Terraform finished successfully, you should see the output with IP
addresses of virtual machines, like:</p>
<pre><code class="language-bash">Apply complete! Resources: 31 added, 0 changed, 0 destroyed.

Outputs:

ip_control_planes = {
  &quot;lockc-control-plane-0&quot; = tolist([
    &quot;10.16.0.113&quot;,
  ])
  &quot;lockc-control-plane-1&quot; = tolist([
    &quot;10.16.0.137&quot;,
  ])
}
ip_workers = {
  &quot;lockc-worker-0&quot; = tolist([
    &quot;10.16.0.174&quot;,
  ])
  &quot;lockc-worker-1&quot; = tolist([
    &quot;10.16.0.44&quot;,
  ])
}

</code></pre>
<p>You can simply ssh to them using the <code>opensuse</code> user:</p>
<pre><code class="language-bash">ssh opensuse@10.16.0.255
</code></pre>
<h2 id="kubernetes"><a class="header" href="#kubernetes">Kubernetes</a></h2>
<p>If you chose to deploy Kubernetes, there will be the <code>admin.conf</code> file
available after successful run of terraform. You can use it by doing:</p>
<pre><code class="language-bash">export KUBECONFIG=$(pwd)/admin.conf
</code></pre>
<p>Then we can check whether Kubernetes is running properly:</p>
<pre><code class="language-bash">$ kubectl get nodes
NAME                    STATUS   ROLES                  AGE   VERSION
lockc-control-plane-0   Ready    control-plane,master   15m   v1.22.3
lockc-control-plane-1   Ready    control-plane,master   14m   v1.22.3
lockc-worker-0          Ready    &lt;none&gt;                 13m   v1.22.3
lockc-worker-1          Ready    &lt;none&gt;                 13m   v1.22.3
$ kubectl get pods -A
NAMESPACE     NAME                                            READY   STATUS    RESTARTS      AGE
kube-system   cilium-6s8m6                                    1/1     Running   0             14m
kube-system   cilium-nnrvx                                    1/1     Running   0             14m
kube-system   cilium-operator-5986db558f-kj9w5                1/1     Running   0             15m
kube-system   cilium-operator-5986db558f-xddgx                1/1     Running   1 (14m ago)   15m
kube-system   cilium-tms8r                                    1/1     Running   1 (14m ago)   15m
kube-system   cilium-x5rtz                                    1/1     Running   0             14m
kube-system   coredns-78fcd69978-cppjl                        1/1     Running   0             14m
kube-system   coredns-78fcd69978-rn58p                        1/1     Running   0             14m
kube-system   etcd-lockc-control-plane-0                      1/1     Running   0             15m
kube-system   etcd-lockc-control-plane-1                      1/1     Running   0             14m
kube-system   kube-apiserver-lockc-control-plane-0            1/1     Running   0             15m
kube-system   kube-apiserver-lockc-control-plane-1            1/1     Running   0             14m
kube-system   kube-controller-manager-lockc-control-plane-0   1/1     Running   1 (14m ago)   15m
kube-system   kube-controller-manager-lockc-control-plane-1   1/1     Running   0             14m
kube-system   kube-proxy-42b5t                                1/1     Running   0             14m
kube-system   kube-proxy-4stzw                                1/1     Running   0             14m
kube-system   kube-proxy-7jtfb                                1/1     Running   0             14m
kube-system   kube-proxy-mhrxq                                1/1     Running   0             15m
kube-system   kube-scheduler-lockc-control-plane-0            1/1     Running   1 (14m ago)   15m
kube-system   kube-scheduler-lockc-control-plane-1            1/1     Running   0             14m
</code></pre>
<p>Now it's time to build and deploy lockc! Detailed instructions how to do that
are in the following documentation sections:</p>
<ul>
<li><a href="terraform/../build/container-image.html">Container image</a></li>
<li><a href="terraform/../install/kubernetes.html">Install on Kubernetes</a></li>
</ul>
<p>But let's continue with a short summary of the easiest way to do that:</p>
<pre><code class="language-bash"># Go to the main directory of lockc sources
cd ../../..
export IMAGE_NAME=$(uuidgen)
docker build -t ttl.sh/${IMAGE_NAME}:30m .
docker push ttl.sh/${IMAGE_NAME}:30m
helm install lockc contrib/helm/lockc/ --namespace kube-system \
    --set lockcd.image.repository=ttl.sh/${IMAGE_NAME} \
    --set lockcd.image.tag=30m \
    --set lockcd.debug.enabled=true
</code></pre>
<p>Then wait until the <code>lockcd</code> DaemonSet is ready:</p>
<pre><code>$ kubectl -n kube-system get ds lockcd
NAME     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
lockcd   4         4         4       4            4           &lt;none&gt;          42s
</code></pre>
<p>You can check further whether lockc is working properly by using example
deployments. Some of them (not violating policies) should deploy successfully.
The other ones (violating policies) should fail.</p>
<p>Let's start with creating namespaces which enforce particular policy levels:</p>
<pre><code class="language-bash">$ cd examples/kubernetes
$ kubectl apply -f namespaces.yaml 
namespace/restricted created
namespace/baseline created
namespace/privileged created
</code></pre>
<p>Then let's deploy examples which should run successfully:</p>
<pre><code class="language-bash">$ kubectl apply -f deployments-should-succeed.yaml 
deployment.apps/nginx-default-success created
deployment.apps/nginx-restricted-success created
deployment.apps/nginx-baseline-success created
deployment.apps/bpf-privileged-success created
$ kubectl get pods -A | grep success
baseline      nginx-baseline-success-8f5dd55f5-8cn9v          1/1     Running   0             17s
default       nginx-default-success-54df89d4ff-582dz          1/1     Running   0             17s
privileged    bpf-privileged-success-5f6b5975b6-hw6h5         1/1     Running   0             17s
restricted    nginx-restricted-success-57b757d5df-btr2s       1/1     Running   0             17s
</code></pre>
<p>And then examples which should fail:</p>
<pre><code class="language-bash">$ kubectl apply -f deployments-should-fail.yaml 
deployment.apps/nginx-restricted-fail created
deployment.apps/bpf-default-fail created
deployment.apps/bpf-restricted-fail created
deployment.apps/bpf-baseline-fail created
$ kubectl get pods -A | grep fail
baseline      bpf-baseline-fail-756b89d76f-gzqzq              0/1     CrashLoopBackOff    5 (17s ago)   3m22s
default       bpf-default-fail-7d767947c7-dvjmt               0/1     RunContainerError   5 (9s ago)    3m22s
restricted    bpf-restricted-fail-7789944bc5-777tq            0/1     CrashLoopBackOff    5 (21s ago)   3m22s
restricted    nginx-restricted-fail-74fb56bb7-vllkx           0/1     ContainerCreating   0             3m22s
</code></pre>
<p>We can dig deeper to check for the reason of failure:</p>
<pre><code class="language-bash">$ kubectl describe pod bpf-default-fail-7d767947c7-dvjmt
[...]
      Message:      failed to create containerd task: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: rootfs_linux.go:76: mounting &quot;/sys/fs/bpf&quot; to rootfs at &quot;/sys/fs/bpf&quot; caused: mount through procfd: operation not permitted: unknown
[...]
</code></pre>
<p>After connecting to one of the VMs we can use bpftool to check lockc BPF
programs:</p>
<pre><code class="language-bash">$ ssh opensuse@10.16.0.174
$ sudo -i
# bpftool prog list
[...]
567: tracing  name sched_process_f  tag fbda76511566c0af  gpl
	loaded_at 2021-11-17T14:49:02+0000  uid 0
	xlated 1528B  jited 869B  memlock 4096B  map_ids 108,107
	btf_id 125
568: lsm  name clone_audit  tag 5cca020a1dcf0cdf  gpl
	loaded_at 2021-11-17T14:49:02+0000  uid 0
	xlated 1600B  jited 899B  memlock 4096B  map_ids 108,107
	btf_id 125
569: tracing  name do_exit  tag 9ebd9eeef8cdbc3a  gpl
	loaded_at 2021-11-17T14:49:02+0000  uid 0
	xlated 488B  jited 285B  memlock 4096B  map_ids 108
	btf_id 125
570: lsm  name syslog_audit  tag 36b3cfb6f1722b24  gpl
	loaded_at 2021-11-17T14:49:02+0000  uid 0
	xlated 1112B  jited 629B  memlock 4096B  map_ids 108,107
	btf_id 125
571: lsm  name mount_audit  tag f972bdc0b55c76aa  gpl
	loaded_at 2021-11-17T14:49:02+0000  uid 0
	xlated 2832B  jited 1620B  memlock 4096B  map_ids 108,107,109,110
	btf_id 125
572: lsm  name open_audit  tag 9617c657c693030f  gpl
	loaded_at 2021-11-17T14:49:02+0000  uid 0
	xlated 2936B  jited 1678B  memlock 4096B  map_ids 108,107,113,114,111,112
	btf_id 125
573: kprobe  name add_container  tag 1bb54a2bc3b00693  gpl
	loaded_at 2021-11-17T14:49:02+0000  uid 0
	xlated 824B  jited 455B  memlock 4096B  map_ids 107,108
	btf_id 125
574: kprobe  name delete_containe  tag a6b9816741d0c7b3  gpl
	loaded_at 2021-11-17T14:49:02+0000  uid 0
	xlated 1104B  jited 639B  memlock 4096B  map_ids 107,108
	btf_id 125
575: kprobe  name add_process  tag ec39a14f8b9b9ccf  gpl
	loaded_at 2021-11-17T14:49:02+0000  uid 0
	xlated 480B  jited 272B  memlock 4096B  map_ids 108
	btf_id 125

[...]
</code></pre>
<p>And whether it registers containers. Directories inside
<code>/sys/fs/bpf/lockc</code> represent timestamps of lockcd launch, so it will be
different than in the following example.</p>
<pre><code class="language-bash"># bpftool map dump pinned /sys/fs/bpf/lockc/1637160542/map_containers 
[{
        &quot;key&quot;: 4257,
        &quot;value&quot;: {
            &quot;policy_level&quot;: &quot;POLICY_LEVEL_BASELINE&quot;
        }
    },{
        &quot;key&quot;: 4780,
        &quot;value&quot;: {
            &quot;policy_level&quot;: &quot;POLICY_LEVEL_BASELINE&quot;
        }
    },{
        &quot;key&quot;: 4664,
        &quot;value&quot;: {
            &quot;policy_level&quot;: &quot;POLICY_LEVEL_RESTRICTED&quot;
        }
    },{
        &quot;key&quot;: 4589,
        &quot;value&quot;: {
            &quot;policy_level&quot;: &quot;POLICY_LEVEL_BASELINE&quot;
        }
    },{
        &quot;key&quot;: 4422,
        &quot;value&quot;: {
            &quot;policy_level&quot;: &quot;POLICY_LEVEL_RESTRICTED&quot;
        }
    }
]
</code></pre>
<h2 id="docker"><a class="header" href="#docker">Docker</a></h2>
<p>Terraform output in case of Docker should look like:</p>
<pre><code class="language-bash">Apply complete! Resources: 9 added, 0 changed, 0 destroyed.

Outputs:

ip_control_planes = {
  &quot;lockc-control-plane-0&quot; = tolist([
    &quot;10.16.0.152&quot;,
  ])
}
ip_workers = {}
</code></pre>
<p>We can simply ssh to the VM:</p>
<pre><code class="language-bash">ssh opensuse@10.16.0.152
</code></pre>
<p>And then try to run some insecure container:</p>
<pre><code class="language-bash">$ docker run --rm -it -v /:/rootfs registry.opensuse.org/opensuse/toolbox:latest bash
docker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: rootfs_linux.go:76: mounting &quot;/&quot; to rootfs at &quot;/rootfs&quot; caused: mount through procfd: operation not permitted: unknown.
</code></pre>
<p>lockc should prevent that container from running. However, running such
container as root with <code>privileged</code> policy level should be fine:</p>
<pre><code class="language-bash">$ sudo -i
# docker run --label org.lockc.policy=privileged --rm -it -v /:/rootfs registry.opensuse.org/opensuse/toolbox:latest bash
8ea310609fce:/ # 
</code></pre>
<h2 id="variables"><a class="header" href="#variables">Variables</a></h2>
<p>Variables available in <code>terraform.tfvars.json</code> are:</p>
<ul>
<li><code>libvirt_uri</code> - libvirt connection URI</li>
<li><code>pool</code> - pool to be used to store all the volumes</li>
<li><code>image_name</code> - image name in libvirt</li>
<li><code>image_path</code> - path or URL to the image</li>
<li><code>network_name</code> - network name in libvirt</li>
<li><code>network_mode</code> - network mode in libvirt (<code>nat</code> / <code>none</code> / <code>route</code> / <code>bridge</code>)</li>
<li><code>dns_domain</code> - DNS domain name</li>
<li><code>stack_name</code> - identifier to make all your resources unique and avoid clashes with other users of this terraform project</li>
<li><code>network_cidr</code> - network CIDR</li>
<li><code>locale</code> - system locales to set onm all the nodes</li>
<li><code>timezone</code> - timezone to set on all the nodes</li>
<li><code>authorized_keys</code> - SSH keys to inject into all the nodes</li>
<li><code>repositories</code> - zypper repositories to add</li>
<li><code>packages</code> - list of additional packages to install#</li>
<li><code>enable_docker</code> - enable Docker support (as a non-clustered container engine</li>
<li><code>enable_k8s_containerd</code> - enable Kubernetes with containerd CRI</li>
<li><code>username</code> - default user in VMs</li>
<li><code>control_planes</code> - number of control plane VMs to create</li>
<li><code>control_plane_memory</code> - the amount of RAM (MB) for a control plane node</li>
<li><code>control_plane_vcpu</code> - the amount of virtual CPUs for a control plane node</li>
<li><code>control_plane_disk_size</code> - disk size (in bytes)</li>
<li><code>workers</code> - number of worker VMs to create</li>
<li><code>worker_memory</code> - the amount of RAM (MB) for a worker node</li>
<li><code>worker_vcpu</code> - the amount of virtual CPUs for a worker node</li>
<li><code>worker_disk_size</code> - disk size (in bytes)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openstack"><a class="header" href="#openstack">OpenStack</a></h1>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p>These terraform definitions are going to create the whole
cluster on top of openstack.</p>
<h2 id="deployment"><a class="header" href="#deployment">Deployment</a></h2>
<p>Make sure to download an openrc file from your OpenStack instance, e.g.:</p>
<p><code>https://engcloud.prv.suse.net/project/api_access/openrc/</code></p>
<p>and source it:</p>
<pre><code class="language-sh">source container-openrc.sh
</code></pre>
<p>Also make sure to have your ssh key within OpenStack, by adding your key to the
key_pairs first.</p>
<p>Upload <code>lockc</code> base image build with guestfs.</p>
<pre><code class="language-sh">openstack image create lockc-`date +%F` --disk-format qcow2 --file ./lockc-base.qcow2
</code></pre>
<p>Once you perform a <a href="terraform/openstack.html#Customization">Customization</a> you can use <code>terraform</code> to deploy the cluster:</p>
<pre><code class="language-sh">terraform init
terraform validate
terraform apply
</code></pre>
<h2 id="machine-access"><a class="header" href="#machine-access">Machine access</a></h2>
<p>It is important to have your public ssh key within the <code>authorized_keys</code>,
this is done by <code>cloud-init</code> through a terraform variable called <code>authorized_keys</code>.</p>
<p>All the instances have a <code>root</code> and <code>opensuse</code> user. The normal 'opensuse' user user can
perform <code>sudo</code> without specifying a password.</p>
<p>Neither root nor the normal <code>opensuse</code> user will have password. Terraform
is using SSH key-based authentication. You can always set a password after the
creation of the machines using <code>sudo passwd opensuse</code> (for normal user) or <code>sudo passwd</code> (for root).</p>
<h2 id="load-balancer"><a class="header" href="#load-balancer">Load balancer</a></h2>
<p>The kubernetes api-server instances running inside of the cluster are
exposed by a load balancer managed by OpenStack.</p>
<h2 id="customization"><a class="header" href="#customization">Customization</a></h2>
<p>Copy the <code>terraform.tfvars.example</code> to <code>terraform.tfvars</code> and
provide reasonable values.</p>
<h2 id="variables-1"><a class="header" href="#variables-1">Variables</a></h2>
<p><code>image_name</code> - Name of the image to use<br />
<code>internal_net</code> - Name of the internal network to be created<br />
<code>stack_name</code> - Identifier to make all your resources unique and avoid clashes with other users of this terraform project<br />
<code>authorized_keys</code> - A list of ssh public keys that will be installed on all nodes<br />
<code>repositories</code> - Additional repositories that will be added on all nodes<br />
<code>packages</code> - Additional packages that will be installed on all nodes\</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="policies"><a class="header" href="#policies">Policies</a></h1>
<p>lockc provides three policy levels for containers:</p>
<ul>
<li><strong>baseline</strong> - meant for regular applications</li>
<li><strong>restricted</strong> - meant for applications for which we need to be more cautious
and secure them more stricly</li>
<li><strong>privileged</strong> - meant for part of the infrastructure which can have full
access to host resources - i.e. CNI plugins in Kubernetes</li>
</ul>
<p>The default policy level is <strong>baseline</strong>. The policy level can be changed by
the <code>pod-security.kubernetes.io/enforce</code> label on the <strong>namespace</strong> which
the container is running in. We make an exception for the <em>kube-system</em>
namespace for which the <strong>privileged</strong> policy is applied by default.</p>
<p>For now there is no possibility to apply policy levels on local container
engines (Docker, containerd, podman), but such feature is planned in the
future.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="file-access"><a class="header" href="#file-access">File access</a></h1>
<p>lockc comes with policies about file access which is based on allow- and
deny-listing. <strong>Baseline</strong> and <strong>restricted</strong> policies have their own pairs of
lists. All those lists should contain path prefixes. All the children of listed
paths/directories are included, since the decision is made by prefix matching.</p>
<p>The deny list has precedence over allow list. That's because main purpose of
the deny list is specifying exceptions whose prefixes are specified in the
allow list, but we don't want to allow them.</p>
<p>To sum it up, when any process in the container tries to access a file, lockc:</p>
<ol>
<li>Checks whether the given path's prefix is in the deny list. If yes, denies
the access.</li>
<li>Checks whether the given path's prefix is in the allow list. If yes, allows
the access.</li>
<li>In case of no matches, denies the access.</li>
</ol>
<p>By default, the contents of lists are:</p>
<ul>
<li><strong>baseline</strong>
<ul>
<li>allow list
<ul>
<li><em>/bin</em></li>
<li><em>/dev/console</em></li>
<li><em>/dev/full</em></li>
<li><em>/dev/null</em></li>
<li><em>/dev/pts</em></li>
<li><em>/dev/tty</em></li>
<li><em>/dev/urandom</em></li>
<li><em>/dev/zero</em></li>
<li><em>/etc</em></li>
<li><em>/home</em></li>
<li><em>/lib</em></li>
<li><em>/proc</em></li>
<li><em>/sys/fs/cgroup</em></li>
<li><em>/tmp</em></li>
<li><em>/usr</em></li>
<li><em>/var</em></li>
</ul>
</li>
<li>deny list
<ul>
<li><em>/proc/acpi</em></li>
</ul>
</li>
</ul>
</li>
<li><strong>restricted</strong>
<ul>
<li>allow list
<ul>
<li><em>/bin</em></li>
<li><em>/dev/console</em></li>
<li><em>/dev/full</em></li>
<li><em>/dev/null</em></li>
<li><em>/dev/pts</em></li>
<li><em>/dev/tty</em></li>
<li><em>/dev/urandom</em></li>
<li><em>/dev/zero</em></li>
<li><em>/etc</em></li>
<li><em>/home</em></li>
<li><em>/lib</em></li>
<li><em>/proc</em></li>
<li><em>/sys/fs/cgroup</em></li>
<li><em>/tmp</em></li>
<li><em>/usr</em></li>
<li><em>/var</em></li>
</ul>
</li>
<li>deny list
<ul>
<li><em>/proc/acpi</em></li>
<li><em>/proc/sys</em></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>By default, with the <strong>baseline</strong> policy level, this is a good exampole of not
allowed behavior:</p>
<pre><code class="language-bash"># docker run --rm -it registry.opensuse.org/opensuse/toolbox:latest
9b34d760017f:/ # ls /sys
ls: cannot open directory '/sys': Operation not permitted
9b34d760017f:/ # ls /sys/fs/btrfs
ls: cannot access '/sys/fs/btrfs': No such file or directory
9b34d760017f:/ # ls /sys/fs/cgroup
blkio  cpu,cpuacct  cpuset   freezer  memory  net_cls           net_prio    pids  systemd
cpu    cpuacct      devices  hugetlb  misc    net_cls,net_prio  perf_event  rdma
</code></pre>
<p>We are able to see cgroups (which is fine), but other parts of <em>/sys</em> are
hidden.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mount-policies"><a class="header" href="#mount-policies">Mount policies</a></h1>
<p>lockc comes with the following policies about bind mounts from host filesystem
to containers (via <code>-v</code> option) for each policy level:</p>
<ul>
<li><strong>baseline</strong> - allow bind mounting from inside <code>/home</code> and <code>/var/data</code>.</li>
<li><strong>restricted</strong> - does not allow any bind mounts from host</li>
<li><strong>privileged</strong> - no restrictions, everything can be bind mounted</li>
</ul>
<p>The <strong>baseline</strong> behavior in lockc is slightly different than in the Kubernetes
Pod Security Admission controller, which disallows all host mounts for baseline
containers as well as for restricted. The motivation behind allowing <code>/home</code>
and <code>/var/data</code> by lockc is that they are often used in local container engines
(Docker, podman) for reasons like:</p>
<ul>
<li>mounting the source code to build or check</li>
<li>storing database content on the host for local development</li>
</ul>
<p>By default, with the <strong>baseline</strong> policy level, this is a good example of
not allowed behavior:</p>
<pre><code class="language-bash"># docker run --rm -it -v /:/rootfs registry.opensuse.org/opensuse/toolbox:latest
docker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: rootfs_linux.go:76: mounting &quot;/&quot; to rootfs at &quot;/rootfs&quot; caused: mount through procfd: operation not permitted: unknown.
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="syslog"><a class="header" href="#syslog">Syslog</a></h1>
<p>lockc comes with the following policies about access to the kernel message ring
buffer for each policy level:</p>
<ul>
<li><strong>baseline</strong> - not allowed</li>
<li><strong>restricted</strong> - not allowed</li>
<li><strong>privileged</strong> - allowed</li>
</ul>
<p>By default, with the <strong>baseline</strong> policy level, checking the kernel logs from
the container is not allowed:</p>
<pre><code class="language-bash"># docker run -it --rm registry.opensuse.org/opensuse/toolbox:latest
b10f9fa4a385:/ # dmesg
dmesg: read kernel buffer failed: Operation not permitted
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tuning"><a class="header" href="#tuning">Tuning</a></h1>
<p>This guide shows options and tricks to gain an optimal performance and resouce
usage.</p>
<h2 id="memory-usage"><a class="header" href="#memory-usage">Memory usage</a></h2>
<p>Memory usage by lockc depends mostly on BPF maps size. BPF maps are stored in
memory and the biggest BPF maps are the ones related to tracking processes and
containers. Size of those maps depends on the limit of processes (in separate
memory spaces) in the system. That limit is determined by the <code>kernel.pid_max</code>
sysctl. By default the limit is 32768. With such limit, memory usage by lockc
should be aproximately 10-20 MB.</p>
<p>If you observe too much memory being used after installing lockc, try to check
the value of <code>kernel.pid_max</code>, which can be done with:</p>
<pre><code class="language-bash">sudo sysctl kernel.pid_max
</code></pre>
<p>Change of that value (i.e. to 10000) can be done with:</p>
<pre><code class="language-bash">sudo sysctl kernel.pid_max=10000
</code></pre>
<p>But that change will be not persistent after reboot. Changing it persistently
requires adding a configuration to <code>/etc/sysctl.d</code>. I.e. we could create the
file <code>/etc/sysctl.d/05-lockc.conf</code> with the following content:</p>
<pre><code>kernel.pid_max = 10000
</code></pre>
<p>After creating that file, the lower limit is going to be persistent after
reboot.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="demos"><a class="header" href="#demos">Demos</a></h1>
<p>This section of the book contains demos.</p>
<ul>
<li><a href="demos/mount.html">Mount</a> - mount policies</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mount-policies-1"><a class="header" href="#mount-policies-1">Mount policies</a></h1>
<h2 id="kubernetes-1"><a class="header" href="#kubernetes-1">Kubernetes</a></h2>
<p>The following demo shows mount policies being enforced on Kubernetes pods.</p>
<p>YAML files can be found <a href="https://github.com/rancher-sandbox/lockc/tree/main/examples/kubernetes">here</a>.</p>
<p>The policy violations in <a href="https://github.com/rancher-sandbox/lockc/tree/main/examples/kubernetes/deployments-should-fail.yaml">deployments-should-fail.yaml</a>
file are:</p>
<ul>
<li><em>nginx-restricted-fail</em> deployment trying to make a host mount while having a
<strong>restricted</strong> policy</li>
<li><em>bpf-default-fail</em> and <em>bpf-baseline-fail</em> deployment trying to mount
<code>/sys/fs/bpf</code> while having a <strong>baseline</strong> policy</li>
<li><em>bpf-restricted-fail</em> trying to mount <code>/sys/fs/bpf</code> while having a
<strong>restricted</strong> policy.</li>
</ul>
<p><a href="https://asciinema.org/a/sUxMMB5BKkJzlF1jP6k8Bxab3"><img src="https://asciinema.org/a/sUxMMB5BKkJzlF1jP6k8Bxab3.svg" alt="asciicast" /></a></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
