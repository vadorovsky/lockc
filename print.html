<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>lockc Documentation</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="**lockc** is open source software for providing MAC (Mandatory Access Control) implemented in Rust">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="containers-do-not-contain.html"><strong aria-hidden="true">2.</strong> Containers do not contain</a></li><li class="chapter-item expanded "><a href="architecture.html"><strong aria-hidden="true">3.</strong> Architecture</a></li><li class="chapter-item expanded "><a href="configuration.html"><strong aria-hidden="true">4.</strong> Getting started</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="build/index.html"><strong aria-hidden="true">4.1.</strong> Build</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="build/dapper.html"><strong aria-hidden="true">4.1.1.</strong> Dapper</a></li><li class="chapter-item expanded "><a href="build/cargo.html"><strong aria-hidden="true">4.1.2.</strong> Cargo</a></li></ol></li><li class="chapter-item expanded "><a href="use/index.html"><strong aria-hidden="true">4.2.</strong> Use</a></li><li class="chapter-item expanded "><a href="terraform/index.html"><strong aria-hidden="true">4.3.</strong> Provision</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="terraform/base-image.html"><strong aria-hidden="true">4.3.1.</strong> Base image</a></li><li class="chapter-item expanded "><a href="terraform/custom-kernel.html"><strong aria-hidden="true">4.3.2.</strong> (Optional) Custom kernel</a></li><li class="chapter-item expanded "><a href="terraform/libvirt.html"><strong aria-hidden="true">4.3.3.</strong> Use libvirt</a></li><li class="chapter-item expanded "><a href="terraform/openstack.html"><strong aria-hidden="true">4.3.4.</strong> Use OpenStack</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="policies/index.html"><strong aria-hidden="true">5.</strong> Policies</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="policies/file-access.html"><strong aria-hidden="true">5.1.</strong> File access</a></li><li class="chapter-item expanded "><a href="policies/mount.html"><strong aria-hidden="true">5.2.</strong> Mount</a></li><li class="chapter-item expanded "><a href="policies/syslog.html"><strong aria-hidden="true">5.3.</strong> Syslog</a></li></ol></li><li class="chapter-item expanded "><a href="tuning/index.html"><strong aria-hidden="true">6.</strong> Tuning</a></li><li class="chapter-item expanded "><a href="demos/index.html"><strong aria-hidden="true">7.</strong> Demos</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="demos/mount.html"><strong aria-hidden="true">7.1.</strong> Mount</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">lockc Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p><img src="/images/logo-horizontal-lockc.png" alt="lockc" /></p>
<h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p><strong>lockc</strong> is open source software for providing MAC (Mandatory Access Control)
type of security audit for container workloads.</p>
<p>The main reason why <strong>lockc</strong> exists is that <strong>containers do not contain</strong>.
Containers are not as secure and isolated as VMs. By default, they expose
a lot of information about host OS and provide ways to &quot;break out&quot; from the
container. <strong>lockc</strong> aims to provide more isolation to containers and make them
more secure.</p>
<p>The <a href="containers-do-not-contain.html">Containers do not contain</a> documentation
section explains why we mean by that phrase and what kind of behavior we want
to restrict with <strong>lockc</strong>.</p>
<p>The main technology behind lockc is <a href="https://ebpf.io/">eBPF</a> - to be more
precise, its ability to attach to <a href="https://www.kernel.org/doc/html/latest/bpf/bpf_lsm.html">LSM hooks</a></p>
<p>Please note that currently lockc is an experimental project, not meant for
production environments. Currently we don't publish any official binaries or
packages to use, except of a Rust crate. Currently the most convenient way
to use it is to use the source code and follow the guide.</p>
<h2 id="contributing"><a class="header" href="#contributing">Contributing</a></h2>
<p>If you need help or want to talk with contributors, please come chat with
us on <code>#lockc</code> channel on the <a href="https://discord.gg/799cmsYB4q">Rust Cloud Native Discord server</a>.</p>
<p>You can find the source code on <a href="https://github.com/rancher-sandbox/lockc">GitHub</a>
and issues and feature requests can be posted on the
<a href="https://github.com/rancher-sandbox/lockc/issues">GitHub issue tracker</a>.
<strong>lockc</strong> relies on the community to fix bugs and add features: if you'd like
to contribute, please read the <a href="https://github.com/rancher-sandbox/lockc/blob/master/CONTRIBUTING.md">CONTRIBUTING</a>
guide and consider opening <a href="https://github.com/rancher-sandbox/lockc/pulls">pull request</a>.</p>
<h2 id="license"><a class="header" href="#license">License</a></h2>
<p><strong>lockc's</strong> userspace part is licensed under <a href="https://github.com/rancher-sandbox/lockc/blob/main/LICENSE">Apache License, version 2.0</a>.</p>
<p>eBPF programs inside <a href="https://github.com/rancher-sandbox/lockc/tree/main/lockc/src/bpf">lockc/src/bpf directory</a>
are licensed under <a href="https://github.com/rancher-sandbox/lockc/blob/main/lockc/src/bpf/LICENSE">GNU General Public License, version 2</a>.</p>
<p>Documentation is licensed under <a href="https://www.mozilla.org/MPL/2.0/">Mozilla Public License v2.0</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="containers-do-not-contain"><a class="header" href="#containers-do-not-contain">Containers do not contain</a></h1>
<p>Many people assume that containers:</p>
<ul>
<li>provide the same or similar isolation to virtual machines</li>
<li>protects the host system</li>
<li>sandboxes applications</li>
</ul>
<p>While all the points except the first one are partially true, some parts of the
host filesystems are still exposed by default to containers and there are ways to
gain full access.</p>
<p>This section highlights and explains problematic exploitation possibilities
that <strong>lockc</strong> aims to fix via policies.</p>
<p>Please note that as <strong>lockc</strong> is still in early development stage, it doesn't
protect against all examples provided at this time. However, covering them all
is in the roadmap.</p>
<p>The goal of <strong>lockc</strong> is to eventually prevent any of those examples to be done
by a regular user. Following some examples as root by explicitly choosing the
<em>privileged</em> policy level in lockc is going to be still allowed. However, it is
is discouraged to use the <em>priviliged</em> level for containers which are not part
of Kubernetes infra (CNI plugins, operators, network meshes etc.). We might
still consider restricting some of behaviors even for <em>privileged</em> (i.e. it's
probably hard to justify <code>chroot</code> inside containers under any ciricumstance).</p>
<h2 id="not-everything-is-namespaced"><a class="header" href="#not-everything-is-namespaced">Not everything is namespaced</a></h2>
<p>Despite the fact that containers come with their own rootfs, some parts of the
filesystem are <strong>not namespaced</strong>, which means that the content of some
directories is <strong>exactly the same as on the host OS</strong>. Examples:</p>
<ul>
<li>Kernel filesystems under <em>/sys</em></li>
<li>many sysctls under <em>/proc/sys</em></li>
</ul>
<p>For non-privileged containers, the content of those directories is read-only.
However, privileged containers can write to them. In both cases, we think that
even exposing many of those directories without write access is unnecessary
for regular containers.</p>
<p>To show some more concrete examples, access to those directories can allow to:</p>
<ul>
<li>Check and change GPU settings</li>
</ul>
<pre><code class="language-bash">❯ docker run --rm -it opensuse/tumbleweed:latest bash
f4891490a2f3:/ # cat /sys/class/drm/card0/device/power_dpm_force_performance_level
auto
f4891490a2f3:/ # exit
❯ docker run --rm --privileged -it opensuse/tumbleweed:latest bash
bad479286479:/ # echo high &gt; /sys/class/drm/card0/device/power_dpm_force_performance_level
bad479286479:/ # cat /sys/class/drm/card0/device/power_dpm_force_performance_level
high
bad479286479:/ # exit
❯ cat /sys/class/drm/card0/device/power_dpm_force_performance_level
high
</code></pre>
<ul>
<li>look at the host OS filesystem metadata</li>
</ul>
<pre><code class="language-bash">❯ docker run --rm -it opensuse/tumbleweed:latest bash
0d35122d08f9:~ # ls /sys/fs/btrfs/a8222a26-d11e-4276-9c38-9df2812cead2/
allocation  bdi  bg_reclaim_threshold  checksum  clone_alignment  devices  devinfo  exclusive_operation  features  generation  label  metadata_uuid  nodesize  qgroups  quota_override  read_policy  sectorsize
</code></pre>
<ul>
<li>use fdisk in a privileged container</li>
</ul>
<pre><code class="language-bash">❯ docker run --rm -it --privileged registry.opensuse.org/opensuse/toolbox:latest bash
8b71e0119552:/ # fdisk -l
Disk /dev/nvme0n1: 1.82 TiB, 2000398934016 bytes, 3907029168 sectors
Disk model: Samsung SSD 970 EVO Plus 2TB
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: gpt
Disk identifier: 8EEBDAB8-F965-4BA0-918A-2671BC67117C

Device           Start        End    Sectors  Size Type
/dev/nvme0n1p1    2048    1026047    1024000  500M EFI System
/dev/nvme0n1p2 1026048 3907029134 3906003087  1.8T Linux filesystem
</code></pre>
<h2 id="host-mounts"><a class="header" href="#host-mounts">Host mounts</a></h2>
<p>Container engines allow to bind mount any directory from the host. When using
local, non-clusterized container engines (docker, podman etc.) there are no
restrictions about what can be mounted. In case of Docker, anyone who has an
access to the socket (usually a member of <code>docker</code> group) can mount anything.</p>
<p>That gives every member of the <code>docker</code> group an access to the host OS as root:</p>
<pre><code class="language-bash">❯ docker run --rm --privileged -it -v /:/rootfs opensuse/tumbleweed:latest bash
efa4f6e0529a:/ # chroot /rootfs
sh-4.4#
</code></pre>
<p>The <code>chroot</code> works without <code>--privileged</code> as well:</p>
<pre><code class="language-bash">❯ docker run --rm -it -v /:/rootfs opensuse/tumbleweed:latest bash
abb67212044d:/ # chroot /rootfs
sh-4.4#
</code></pre>
<p>The other approach is to mount a Docker socket. The image used here is <code>docker</code>
which is the official image with Docker binaries installed. After starting the
first container, we are able to list containers running on the host. Then, we
are able to run another container - from inside the first one - which is
mounting directories from the host</p>
<pre><code class="language-bash">❯ docker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock docker sh
/ # docker ps
CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS     NAMES
066811b60d69   docker    &quot;docker-entrypoint.s…&quot;   5 seconds ago   Up 5 seconds             suspicious_liskov
/ # docker run --rm --privileged -it opensuse/tumbleweed:latest bash
fcb94c1d3af6:/ # exit
/ # docker run --rm --privileged -it -v /:/rootfs opensuse/tumbleweed:latest bash
54b08e30fd9e:/ # chroot /rootfs
sh-4.4# cat /etc/os-release
NAME=&quot;openSUSE Leap&quot;
VERSION=&quot;15.3&quot;
ID=&quot;opensuse-leap&quot;
ID_LIKE=&quot;suse opensuse&quot;
VERSION_ID=&quot;15.3&quot;
PRETTY_NAME=&quot;openSUSE Leap 15.3&quot;
ANSI_COLOR=&quot;0;32&quot;
CPE_NAME=&quot;cpe:/o:opensuse:leap:15.3&quot;
BUG_REPORT_URL=&quot;https://bugs.opensuse.org&quot;
HOME_URL=&quot;https://www.opensuse.org/&quot;
</code></pre>
<p>Notice the difference between Linux distibution versions. The second container
image we used is <em>openSUSE Tumbleweed</em>, but the host is running
<em>openSUSE Leap 15.3</em>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="architecture"><a class="header" href="#architecture">Architecture</a></h1>
<p>The project consists of 3 parts:</p>
<ul>
<li>the set of BPF programs (written in C)
<ul>
<li>programs for monitoring processes, which detects whether new processes
are running inside any container, which means applying policies on them</li>
<li>programs attached to particular LSM hooks, which allow or deny actions
based on the policy applied to the container (currently all containers have
the <code>baseline</code> policy applied, the mechanism of differentiating between
policies per container/pod is yet to be implemented)</li>
</ul>
</li>
<li><strong>lockcd</strong> - the userspace program (written in Rust)
<ul>
<li>loads the BPF programs into the kernel, pins them in BPFFS</li>
<li>in future, it's going to serve as the configuration manager and log
collector</li>
</ul>
</li>
<li><strong>lockc-runc-wrapper</strong> - a wraper for runc which registers new containers
and determines which policy should be applied on a container</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting started</a></h1>
<ul>
<li><a href="build/README.html">Build</a> - How to build lockc from the sources</li>
<li><a href="use/README.html">Use</a> - Configuring and using lockc locally</li>
<li><a href="terraform/README.html">Provision</a> - :Using Terraform for provisioning lockc</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="building-lockc"><a class="header" href="#building-lockc">Building lockc</a></h2>
<p>The first step to try out lockc is to build it. There are two ways to do
that:</p>
<ul>
<li><strong><a href="build/dapper.html">Dapper</a></strong> - build binaries within container
<ul>
<li>doesn't require any dependencies on the host system (except Docker)</li>
<li>ensures that Rust, Cargo and all dependencies are in the newest version</li>
<li>ensures the same behavior as on CI, aims to reduce &quot;it worked on machine&quot;
kind of problems</li>
<li>recommended for
<ul>
<li>trying out the project (especially if there is no interest in changing
the code)</li>
<li>final build before submitting changes in the code</li>
</ul>
</li>
</ul>
</li>
<li><strong><a href="build/cargo.html">Cargo</a></strong> - build binaries with Cargo (Rust build system) on the host
<ul>
<li>convenient for local development, IDE/editor integration</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h3 id="dapper"><a class="header" href="#dapper">Dapper</a></h3>
<p>One option for building lockc is using dapper to perform the build inside
container, without installing needed dependencies on the host system.</p>
<p>This guide assumes that you have <code>docker</code> or any other container engine
installed.</p>
<p>The first step is to install dapper, if it's not present. It can be done
either by downloading a binary:</p>
<pre><code class="language-bash">curl -sL https://releases.rancher.com/dapper/latest/dapper-$(uname -s)-$(uname -m) &gt; /usr/local/bin/dapper
chmod +x /usr/local/bin/dapper
</code></pre>
<p>Or by using <code>go</code>:</p>
<pre><code class="language-bash">go install github.com/rancher/dapper@latest
</code></pre>
<p>Dapper should be launched always in the main directory of the project, where
<code>Dockerfile.dapper</code> file is present.</p>
<p>Our build container image has no entrypoint, so calling <code>dapper</code> without any
argument is spawning a shell inside the container:</p>
<pre><code class="language-bash">$ dapper
[...]
root@ea133ef3d28e:/source#
</code></pre>
<p>Usually we will be interested in using <code>cargo</code> inside the container spawned by
dapper.
<a href="build/cargo.html">More information about cargo can be found here.</a></p>
<p>The build (of both BPF and userspace part) can be performed by running the
following command:</p>
<pre><code class="language-bash">dapper cargo build
</code></pre>
<p>A successful build should result in binaries being present in <code>target/debug</code>
directory.</p>
<p>Running tests:</p>
<pre><code class="language-bash">dapper cargo test
</code></pre>
<p>Running lints:</p>
<pre><code class="language-bash">dapper cargo clippy
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h3 id="cargo"><a class="header" href="#cargo">Cargo</a></h3>
<p>If you are comfortable with installing all dependencies on your host system,
you need to install the following software:</p>
<ul>
<li>LLVM</li>
<li>libbpf, bpftool</li>
<li>Rust, Cargo</li>
</ul>
<h4 id="llvm"><a class="header" href="#llvm">LLVM</a></h4>
<p>We need a recent version of LLVM (at least 12) to build BPF programs.</p>
<p>LLVM has an official <a href="https://apt.llvm.org/">apt repository</a> with recent
stable versions.</p>
<p>Distributions with up to date software repositories like Arch, Fedora, openSUSE
Tumbleweed are shipping recent versions of LLVM.</p>
<p>In more stable and not up to date distributions (CentOS, openSUSE Leap, RHEL,
SLES), using some kind of development repository might be an option. For
example, openSUSE Leap users can use the following devel repo:</p>
<pre><code class="language-bash">zypper ar -r -p 90 https://download.opensuse.org/repositories/devel:/tools:/compiler/openSUSE_Leap_15.3/devel:tools:compiler.repo
zypper ref
zypper up --allow-vendor-change
zypper in clang llvm
</code></pre>
<p>If there is no packaging of recent LLVM versions for your distribution, there
is also an option to <a href="https://releases.llvm.org/download.html">download binaries</a>.</p>
<h4 id="libbpf-bpftool"><a class="header" href="#libbpf-bpftool">libbpf, bpftool</a></h4>
<p>libbpf is the official C library for writing, loading and managing BPF programs
and entities. bpftool is the official CLI for interacting with BPF subsystem.</p>
<p>Distributions with up to date software (Arch, Fedora, openSUSE Tumbleweed)
usually provide packaging for both.</p>
<p>Especially for more stable and less up to date distributions, but even
generally, we would recommend to build both dependencies from source. Both of
them are the part of the Linux kernel source.</p>
<p>The easiest way to get the kernel source is to download a tarball available on
<a href="https://www.kernel.org/">kernel.org</a>. Then build and install tools from it
(the version might vary from this snippet):</p>
<pre><code class="language-bash">tar -xvf linux-5.14.9.tar.xz
cd linux-5.14.9
cd tools/lib/bpf
make -j $(nproc)
make install prefix=/usr
cd ../../bpf/bpftool
make -j $(nproc)
make install prefix=/usr
</code></pre>
<p>If you are interested in tracking the history of Linux kernel source and/or are
comfortable using git for it, you can clone one of the git trees:</p>
<ul>
<li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/">stable tree</a> -
stable releases and release candidates, this is where the tarball comes from</li>
<li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/">mainline tree</a> -
patches accepted by Linus, release candidates</li>
<li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next.git/">bpf-next tree</a> -
development of BPF features, before being mainlined</li>
<li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/">bpf tree</a> -
BPF bugfixes which are backported to the stable tree</li>
</ul>
<p>Assuming you want to use the stable tree:</p>
<pre><code class="language-bash">git clone git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git
cd linux
git tag -l # List available tags
git checkout v5.14.9 # Check out to whatever is the newest
cd tools/lib/bpf
make -j $(nproc)
make install prefix=/usr
cd ../../bpf/bpftool
make -j $(nproc)
make install prefix=/usr
</code></pre>
<h4 id="installing-rust"><a class="header" href="#installing-rust">Installing Rust</a></h4>
<p>Our recommended way of installing Rust is using <strong>rustup</strong>.
<a href="https://rustup.rs/">Their website</a> contains installation instruction.</p>
<p>After installing rustup, let's install lint tools:</p>
<pre><code class="language-bash">rustup component add clippy rustfmt
</code></pre>
<p>And then cargo-libbpf, needed for building the BPF part:</p>
<pre><code class="language-bash">cargo install libbpf-cargo
</code></pre>
<h4 id="building-lockc-1"><a class="header" href="#building-lockc-1">Building lockc</a></h4>
<p>After installing all needed dependencies, it's time to build lockc.</p>
<p>The build of the project can be done with:</p>
<pre><code class="language-bash">cargo build
</code></pre>
<p>Running tests:</p>
<pre><code class="language-bash">cargo test
</code></pre>
<p>Running lints:</p>
<pre><code class="language-bash">cargo clippy
</code></pre>
<h4 id="installing-lockc"><a class="header" href="#installing-lockc">Installing lockc</a></h4>
<p>To install lockc on your host, use the following command:</p>
<pre><code class="language-bash">cargo xtask install
</code></pre>
<p>Do not run this command with sudo! Why?</p>
<p>tl;dr: you will be asked for password when necessary, don't worry!</p>
<p>Explanation: Running cargo with sudo ends with weird consequences like not
seing cargo content from your home directory or leaving some files owned by
root in <code>target</code>. When any destination directory is owned by root, sudo will
be launched automatically by <code>xtask install</code> just to perform necessary
installation steps.</p>
<p>By default it tries to install lockc binaries in <code>/usr/local/bin</code>, but the
destination directory can be changed by the following arguments:</p>
<ul>
<li><code>--destdir</code> - the rootfs of your system, default: <code>/</code></li>
<li><code>--prefix</code> - prefix of the most of installation destinations, default:
<code>usr/local</code></li>
<li><code>--bindir</code> - directory for binary files, default: <code>bin</code></li>
<li><code>--unitdir</code> - directory for systemd units, default: <code>lib/systemd/system</code></li>
<li><code>--sysconfdir</code> - directory for configuration files, default: <code>etc</code></li>
</ul>
<p>By default, binaries are installed from the <code>debug</code> target profile. If you want
to change it, use the <code>--profile</code> argument. <code>--profile release</code> is what you
most likely want to use when packaging or installing on the production system.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="using-lockc-locally"><a class="header" href="#using-lockc-locally">Using lockc locally</a></h2>
<p>lockc can be used on the local system if you want to secure your local
container engine (docker, podman).</p>
<p>BPF programs can be loaded by executing <code>lockcd</code>:</p>
<p>First, we need to load BPF programs by running lockcd. That can be done
by the following command:</p>
<pre><code class="language-bash">sudo ./build/src/lockcd
</code></pre>
<p>If you have <code>bpftool</code> available on your host, you canm check whether lockc
BPF programs are running. The correct output should look similar to:</p>
<pre><code class="language-bash">sudo bpftool prog
[...]
25910: tracing  name sched_process_f  tag 3a6a6e4defce95ab  gpl
        loaded_at 2021-06-02T16:52:57+0200  uid 0
        xlated 2160B  jited 1137B  memlock 4096B  map_ids 14781,14782,14783
        btf_id 18711
25911: lsm  name clone_audit  tag fc30a5b3e6a4610b  gpl
        loaded_at 2021-06-02T16:52:57+0200  uid 0
        xlated 2280B  jited 1196B  memlock 4096B  map_ids 14781,14782,14783
        btf_id 18711
25912: lsm  name syslog_audit  tag 2cdd93e75fa0e936  gpl
        loaded_at 2021-06-02T16:52:57+0200  uid 0
        xlated 816B  jited 458B  memlock 4096B  map_ids 14783,14782
        btf_id 18711
</code></pre>
<p>To check if containers get &quot;hardened&quot; by lockc, check if you are able to
see the kernel logs from inside the container wrapped by <strong>lockc-runc-wrapper</strong>.
Example:</p>
<pre><code class="language-bash">podman --runtime ./out/lockc-runc-wrapper run -ti --rm registry.opensuse.org/opensuse/toolbox:latest
a135dbc3ef08:/ # dmesg
dmesg: read kernel buffer failed: Operation not permitted
</code></pre>
<p>That error means that lockc works, applied the <em>baseline</em> policy on a new
container and prevented containerized processes from accessing kernel logs.</p>
<p>If<code>dmesg</code> ran successfully and shows the kernel logs, it means that something
went wrong and lockc is not working properly.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="terraform"><a class="header" href="#terraform">Terraform</a></h2>
<p>There is also a possibility to run lockc in virtual machines with
Kubernetes.</p>
<p>In order to do that, ensure that you have the following software installed:</p>
<ul>
<li>libvirt</li>
<li>guestfs-tools</li>
</ul>
<p>Then we can proceed with following steps:</p>
<ul>
<li><strong><a href="terraform/base-image.html">Base image</a></strong> - The first step is to build the VM image</li>
<li><strong><a href="terraform/custom-kernel.html">Custom kernel</a></strong> <em>Optional</em> - building the image with a custom kernel</li>
<li><strong><a href="terraform/libvirt.html">Use libvirt</a></strong> - Configure and start VMs in libvirt environment</li>
<li><strong><a href="terraform/openstack.html">Use OpenStack</a></strong> - Starting VMs in OpenStack environment</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h4 id="building-the-base-image"><a class="header" href="#building-the-base-image">Building the base image</a></h4>
<p>The first step is to build the VM image.</p>
<pre><code class="language-bash">cd contrib/guestfs
./build
</code></pre>
<p>If the script ran successfully, <code>lockc-base.qcow2</code> file should be present.
It cointains the base VM image which will be used by Terraform.</p>
<div style="break-before: page; page-break-before: always;"></div><h4 id="optional-building-the-image-with-a-custom-kernel"><a class="header" href="#optional-building-the-image-with-a-custom-kernel">(Optional) building the image with a custom kernel</a></h4>
<p>The <code>build.sh</code> script can be also used to create a VM image with a
custom kernel if there is a need for kernel testing. You can optionally
provide a path to your kernel source tree. Please note that the kernel
should be already build on the host with <code>make</code>. Our guestfs scripts do
only <code>make modules_install install</code> to install the kernel image and
modules inside a VM. Installing the custom kernel is enabled by using
the <code>CUSTOM_KERNEL</code> environment variable. Its value has to be set to
<code>true</code>. By default, the script assumes that your kernel tree is in
<code>~/linux</code> directory. You can provide a custom path by another
environment variable - <code>KERNEL_SOURCE</code>. Examples of usage:</p>
<pre><code class="language-bash">CUSTOM_KERNEL=true ./build.sh
CUSTOM_KERNEL=true KERNEL_SOURCE=${HOME}/my-git-repos/linux ./build.sh
</code></pre>
<p>If you already used <code>build.sh</code> once and you would like to inject a
custom kernel into already build qcow2 image, there is a separate script</p>
<ul>
<li><code>reinstall-custom-kernel.sh</code>. It takes an optional <code>KERNEL_SOURCE</code>
environment variable. Examples of usage:</li>
</ul>
<pre><code class="language-bash">./reinstall-custom-kernel.sh
KERNEL_SOURCE=${HOME}/my-git-repos/linux ./reinstall-custom-kernel.sh
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h4 id="configure-libvirt"><a class="header" href="#configure-libvirt">Configure libvirt</a></h4>
<p>VMs which we are going to run are using 9p to mount the source tree. To
ensure that those mounts are going to work correctly, open the
<code>/etc/libvirt/qemu.conf</code> file and ensure that the following options
are present there:</p>
<pre><code class="language-bash">user = &quot;root&quot;
group = &quot;root&quot;
dynamic_ownership = 0
</code></pre>
<p>If you had to edit the configuration, save the file and restart libvirt:</p>
<pre><code class="language-bash">sudo systemctl restart libvirtd
</code></pre>
<h4 id="running-vms"><a class="header" href="#running-vms">Running VMs</a></h4>
<p>Now it's time to prepare Terraform environment.</p>
<pre><code class="language-bash">cd contrib/terraform/libvirt
cp terraform.tfvars.json.example terraform.tfvars.json
</code></pre>
<p>After that, open the <code>terraform.tfvars.json</code> file with your favorite text
editor. The only setting which you really need to change is
<code>authorized_keys</code>. Please paste your public SSH key there. Otherwise,
connecting to VMs with SSH will be impossible.</p>
<p>Initialize the environment with:</p>
<pre><code class="language-bash">terraform init
</code></pre>
<p>And then start the VMs:</p>
<pre><code class="language-bash">terraform apply
</code></pre>
<p>Terraform finished successfully, you should see the output with IP
addresses of virtual machines, like:</p>
<pre><code class="language-bash">ip_control_planes = {
  &quot;lockc-control-plane-0&quot; = &quot;10.16.0.225&quot;
}
</code></pre>
<p>You can simply ssh to them using the <code>opensuse</code> user:</p>
<pre><code class="language-bash">ssh opensuse@10.16.0.255
</code></pre>
<p>Inside the VM we can check whether Kubernetes is running:</p>
<pre><code class="language-bash"># kubectl get pods -A
NAMESPACE     NAME                                            READY
STATUS    RESTARTS   AGE
kube-system   coredns-78fcd69978-lvshz                        0/1
Running   0          7s
kube-system   coredns-78fcd69978-q874s                        0/1
Running   0          7s
kube-system   etcd-lockc-control-plane-0                      1/1
Running   0          11s
kube-system   kube-apiserver-lockc-control-plane-0            1/1
Running   0          10s
kube-system   kube-controller-manager-lockc-control-plane-0   1/1
Running   0          11s
kube-system   kube-proxy-p7nrd                                1/1
Running   0          7s
kube-system   kube-scheduler-lockc-control-plane-0            1/1
Running   0          11s
</code></pre>
<p>And whether lockc is running. The main service can be checked by:</p>
<pre><code class="language-bash">systemctl status lockcd
</code></pre>
<p>We can check also whether lockc's BPF programs are running:</p>
<pre><code class="language-bash"># bpftool prog list
35: tracing  name sched_process_f  tag b3c2c2a08effc879  gpl
      loaded_at 2021-08-10T12:23:55+0000  uid 0
      xlated 1528B  jited 869B  memlock 4096B  map_ids 3,2
      btf_id 95
36: lsm  name clone_audit  tag 33a5e8a5da485fd4  gpl
      loaded_at 2021-08-10T12:23:55+0000  uid 0
      xlated 1600B  jited 899B  memlock 4096B  map_ids 3,2
      btf_id 95
37: lsm  name syslog_audit  tag 80d655f557922055  gpl
      loaded_at 2021-08-10T12:23:55+0000  uid 0
      xlated 1264B  jited 714B  memlock 4096B  map_ids 3,2
      btf_id 95
[...]
</code></pre>
<p>And whether it registers containers. Directories inside
<code>/sys/fs/bpf/lockc</code> represent timestamps of lockcd launch, so it will be
different than in the following example.</p>
<pre><code class="language-bash"># bpftool map dump pinned /sys/fs/bpf/lockc/1628598193/map_containers
[{
        &quot;key&quot;: 4506,
        &quot;value&quot;: {
            &quot;policy_level&quot;: &quot;POLICY_LEVEL_PRIVILEGED&quot;
        }
[...]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p>These terraform definitions are going to create the whole
cluster on top of openstack.</p>
<h2 id="deployment"><a class="header" href="#deployment">Deployment</a></h2>
<p>Make sure to download an openrc file from your OpenStack instance, e.g.:</p>
<p><code>https://engcloud.prv.suse.net/project/api_access/openrc/</code></p>
<p>and source it:</p>
<pre><code class="language-sh">source container-openrc.sh
</code></pre>
<p>Also make sure to have your ssh key within OpenStack, by adding your key to the
key_pairs first.</p>
<p>Upload <code>lockc</code> base image build with guestfs.</p>
<pre><code class="language-sh">openstack image create lockc-`date +%F` --disk-format qcow2 --file ./lockc-base.qcow2
</code></pre>
<p>Once you perform a <a href="terraform/openstack.html#Customization">Customization</a> you can use <code>terraform</code> to deploy the cluster:</p>
<pre><code class="language-sh">terraform init
terraform validate
terraform apply
</code></pre>
<h2 id="machine-access"><a class="header" href="#machine-access">Machine access</a></h2>
<p>It is important to have your public ssh key within the <code>authorized_keys</code>,
this is done by <code>cloud-init</code> through a terraform variable called <code>authorized_keys</code>.</p>
<p>All the instances have a <code>root</code> and <code>opensuse</code> user. The normal 'opensuse' user user can
perform <code>sudo</code> without specifying a password.</p>
<p>Neither root nor the normal <code>opensuse</code> user will have password. Terraform
is using SSH key-based authentication. You can always set a password after the
creation of the machines using <code>sudo passwd opensuse</code> (for normal user) or <code>sudo passwd</code> (for root).</p>
<h2 id="load-balancer"><a class="header" href="#load-balancer">Load balancer</a></h2>
<p>The kubernetes api-server instances running inside of the cluster are
exposed by a load balancer managed by OpenStack.</p>
<h2 id="customization"><a class="header" href="#customization">Customization</a></h2>
<p>Copy the <code>terraform.tfvars.example</code> to <code>terraform.tfvars</code> and
provide reasonable values.</p>
<h2 id="variables"><a class="header" href="#variables">Variables</a></h2>
<p><code>image_name</code> - Name of the image to use<br />
<code>internal_net</code> - Name of the internal network to be created<br />
<code>stack_name</code> - Identifier to make all your resources unique and avoid clashes with other users of this terraform project<br />
<code>authorized_keys</code> - A list of ssh public keys that will be installed on all nodes<br />
<code>repositories</code> - Additional repositories that will be added on all nodes<br />
<code>packages</code> - Additional packages that will be installed on all nodes\</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="policies"><a class="header" href="#policies">Policies</a></h1>
<p>lockc provides three policy levels for containers:</p>
<ul>
<li><strong>baseline</strong> - meant for regular applications</li>
<li><strong>restricted</strong> - meant for applications for which we need to be more cautious
and secure them more stricly</li>
<li><strong>privileged</strong> - meant for part of the infrastructure which can have full
access to host resources - i.e. CNI plugins in Kubernetes</li>
</ul>
<p>The default policy level is <strong>baseline</strong>. The policy level can be changed by
the <code>pod-security.kubernetes.io/enforce</code> label on the <strong>namespace</strong> which
the container is running in. We make an exception for the <em>kube-system</em>
namespace for which the <strong>privileged</strong> policy is applied by default.</p>
<p>For now there is no possibility to apply policy levels on local container
engines (Docker, containerd, podman), but such feature is planned in the
future.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="file-access"><a class="header" href="#file-access">File access</a></h2>
<p>lockc comes with policies about file access which is based on allow- and
deny-listing. <strong>Baseline</strong> and <strong>restricted</strong> policies have their own pairs of
lists. All those lists should contain path prefixes. All the children of listed
paths/directories are included, since the decision is made by prefix matching.</p>
<p>The deny list has precedence over allow list. That's because main purpose of
the deny list is specifying exceptions whose prefixes are specified in the
allow list, but we don't want to allow them.</p>
<p>To sum it up, when any process in the container tries to access a file, lockc:</p>
<ol>
<li>Checks whether the given path's prefix is in the deny list. If yes, denies
the access.</li>
<li>Checks whether the given path's prefix is in the allow list. If yes, allows
the access.</li>
<li>In case of no matches, denies the access.</li>
</ol>
<p>By default, the contents of lists are:</p>
<ul>
<li><strong>baseline</strong>
<ul>
<li>allow list
<ul>
<li><em>/bin</em></li>
<li><em>/dev/console</em></li>
<li><em>/dev/full</em></li>
<li><em>/dev/null</em></li>
<li><em>/dev/pts</em></li>
<li><em>/dev/tty</em></li>
<li><em>/dev/urandom</em></li>
<li><em>/dev/zero</em></li>
<li><em>/etc</em></li>
<li><em>/home</em></li>
<li><em>/lib</em></li>
<li><em>/proc</em></li>
<li><em>/sys/fs/cgroup</em></li>
<li><em>/tmp</em></li>
<li><em>/usr</em></li>
<li><em>/var</em></li>
</ul>
</li>
<li>deny list
<ul>
<li><em>/proc/acpi</em></li>
</ul>
</li>
</ul>
</li>
<li><strong>restricted</strong>
<ul>
<li>allow list
<ul>
<li><em>/bin</em></li>
<li><em>/dev/console</em></li>
<li><em>/dev/full</em></li>
<li><em>/dev/null</em></li>
<li><em>/dev/pts</em></li>
<li><em>/dev/tty</em></li>
<li><em>/dev/urandom</em></li>
<li><em>/dev/zero</em></li>
<li><em>/etc</em></li>
<li><em>/home</em></li>
<li><em>/lib</em></li>
<li><em>/proc</em></li>
<li><em>/sys/fs/cgroup</em></li>
<li><em>/tmp</em></li>
<li><em>/usr</em></li>
<li><em>/var</em></li>
</ul>
</li>
<li>deny list
<ul>
<li><em>/proc/acpi</em></li>
<li><em>/proc/sys</em></li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="mount-policies"><a class="header" href="#mount-policies">Mount policies</a></h2>
<p>lockc comes with the following policies about bind mounts from host filesystem
to containers (via <code>-v</code> option) for each policy level:</p>
<ul>
<li><strong>baseline</strong> - allow bind mounting from inside <code>/home</code> and <code>/var/data</code>.</li>
<li><strong>restricted</strong> - does not allow any bind mounts from host</li>
<li><strong>privileged</strong> - no restrictions, everything can be bind mounted</li>
</ul>
<p>The <strong>baseline</strong> behavior in lockc is slightly different than in the Kubernetes
Pod Security Admission controller, which disallows all host mounts for baseline
containers as well as for restricted. The motivation behind allowing <code>/home</code>
and <code>/var/data</code> by lockc is that they are often used in local container engines
(Docker, podman) for reasons like:</p>
<ul>
<li>mounting the source code to build or check</li>
<li>storing database content on the host for local development</li>
</ul>
<p>By default, with the <strong>baseline</strong> policy level, this is a good example of
not allowed behavior:</p>
<pre><code class="language-bash"># podman --runtime $(pwd)/build/src/lockc-runc-wrapper run -ti -v /:/rootfs --rm registry.opensuse.org/opensuse/toolbox:latest
Error: container create failed (no logs from conmon): EOF
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="syslog"><a class="header" href="#syslog">Syslog</a></h2>
<p>lockc comes with the following policies about access to the kernel message ring
buffer for each policy level:</p>
<ul>
<li><strong>baseline</strong> - not allowed</li>
<li><strong>restricted</strong> - not allowed</li>
<li><strong>privileged</strong> - allowed</li>
</ul>
<p>By default, with the <strong>baseline</strong> policy level, checking the kernel logs from
the container is not allowed:</p>
<pre><code class="language-bash"># podman --runtime $(pwd)/build/src/lockc-runc-wrapper run -ti --rm registry.opensuse.org/opensuse/toolbox:latest
b10f9fa4a385:/ # dmesg
dmesg: read kernel buffer failed: Operation not permitted
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="tuning"><a class="header" href="#tuning">Tuning</a></h2>
<p>This guide shows options and tricks to gain an optimal performance and resouce
usage.</p>
<h3 id="memory-usage"><a class="header" href="#memory-usage">Memory usage</a></h3>
<p>Memory usage by lockc depends mostly on BPF maps size. BPF maps are stored in
memory and the biggest BPF maps are the ones related to tracking processes and
containers. Size of those maps depends on the limit of processes (in separate
memory spaces) in the system. That limit is determined by the <code>kernel.pid_max</code>
sysctl. By default the limit is 32768. With such limit, memory usage by lockc
should be aproximately 10-20 MB.</p>
<p>If you observe too much memory being used after installing lockc, try to check
the value of <code>kernel.pid_max</code>, which can be done with:</p>
<pre><code class="language-bash">sudo sysctl kernel.pid_max
</code></pre>
<p>Change of that value (i.e. to 10000) can be done with:</p>
<pre><code class="language-bash">sudo sysctl kernel.pid_max=10000
</code></pre>
<p>But that change will be not persistent after reboot. Changing it persistently
requires adding a configuration to <code>/etc/sysctl.d</code>. I.e. we could create the
file <code>/etc/sysctl.d/05-lockc.conf</code> with the following content:</p>
<pre><code>kernel.pid_max = 10000
</code></pre>
<p>After creating that file, the lower limit is going to be persistent after
reboot.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="demos"><a class="header" href="#demos">Demos</a></h1>
<p>This section of the book contains demos.</p>
<ul>
<li><a href="demos/mount.html">Mount</a> - mount policies</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="mount-policies-1"><a class="header" href="#mount-policies-1">Mount policies</a></h2>
<h3 id="kubernetes"><a class="header" href="#kubernetes">Kubernetes</a></h3>
<p>The following demo shows mount policies being enforced on Kubernetes pods.</p>
<p>YAML files can be found <a href="https://github.com/rancher-sandbox/lockc/tree/main/examples/kubernetes">here</a>.</p>
<p>The policy violations in <a href="https://github.com/rancher-sandbox/lockc/tree/main/examples/kubernetes/deployments-should-fail.yaml">deployments-should-fail.yaml</a>
file are:</p>
<ul>
<li><em>nginx-restricted-fail</em> deployment trying to make a host mount while having a
<strong>restricted</strong> policy</li>
<li><em>bpf-default-fail</em> and <em>bpf-baseline-fail</em> deployment trying to mount
<code>/sys/fs/bpf</code> while having a <strong>baseline</strong> policy</li>
<li><em>bpf-restricted-fail</em> trying to mount <code>/sys/fs/bpf</code> while having a
<strong>restricted</strong> policy.</li>
</ul>
<p><a href="https://asciinema.org/a/sUxMMB5BKkJzlF1jP6k8Bxab3"><img src="https://asciinema.org/a/sUxMMB5BKkJzlF1jP6k8Bxab3.svg" alt="asciicast" /></a></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
